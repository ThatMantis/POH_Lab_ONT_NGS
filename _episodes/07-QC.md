---
title: "6. Quality Check"
teaching: 30
source: md
questions:
- "How to perform QC on sequenced data?"
- "How to filter data that failed QC?"
objectives:
- "Perform QC on sequenced data."
- "Perform filtering on sequenced data."
---

### Quality Check

Usually the initial step after obtaining the raw FASTQ data is a quality control (QC) step. Doing so will give us some details of our sequencing data, such as the length and quality score (Qscore) distributions; as well as highlight potential problems with our sample, library preparation and/or the sequencing run itself. Some of these information can already be found in the MinKNOW app itself during/post sequenincg. However specialised QC tools and programs will provide more information. While there might be many tools available out there, in this workshop we will explore [FastQC] and [PycoQC].

#### FastQC

Be aware that the Over-represented sequences feature may return false results for small datasets or highly repetitive genomes/sequences. Just because FastQC flags it does not mean it has to be trimmed. As usual, tools give you an indication of potential problems to help you make a decision. Never trust a computer


#### PycoQC



### Trimming

After conducting QC above, we can then perform trimming of reads based on the QC reports. Trimming can include filtering and cropping of short or low quality reads, removal of adapter sequencers, and also trimming low-quality parts of reads etc.

However (thankfully?) for most use cases, it might not be necessary to conduct our own further downstream data trimming. ONT's Barcodes and any adapters added during library preparation can/are automatically clipped and trimmed from the raw FASTQ data post sequencing during Basecalling in the MinKNOW app. The MinKNOW app also handles Qscore filtering and length filtering in real-time during sequencing. In the EPI2ME Applications which most users in this workshop will likely use for initial further downstream data processing (such as for De Novo plasmid assembly, or for custom alignment workflows), users can easily choose to filter the reads used for analysis by length and Qscores as well.

Nevertheless, if users will still like to filter and trim their data manually, such as by length, they can run simple commands such as the one shown below from the Bash command line. 
1. `length(seq) >= 100`: trims off reads below 100bp.
2. `length(seq) <= 300`: trims off reads above 300bp.
3. `your.fastq`: input raw data file in FASTQ format.
4. `filtered.fastq`: output trimmed data file in FASTQ format.

~~~
awk 'BEGIN {FS = "\t" ; OFS = "\n"} {header = $0 ; getline seq ; getline qheader ; getline qseq ; if (length(seq) >= 100 && length(seq) <= 300) {print header, seq, qheader, qseq}}' your.fastq > filtered.fastq
~~~
{: .bash}

If users will still like to do triming and filtering of their raw data, such as by cropping off the head and tail sequences (which according to [some papers], generally posses lower Qscores for the first (head) and last (tail) ~130bp), they can do so with command line programs such as [nanofilt] and [chopper]. This head and tail cropping should be done at their own discretion. Users should also understand what they are trimming off, as blindly trimming and cropping off reads could potentially result in the loss of true variations and details.

Nevertheless, I personally find that for most of my applications (in microbiology, and relatively shorter read sequencing), especially when involving EPI2ME as part of the workflow, trimming is not required at all :) .


[FastQC]: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/
[PycoQC]: https://a-slide.github.io/pycoQC/
[some papers]: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0257521
[nanofilt]: https://github.com/wdecoster/nanofilt
[chopper]: https://github.com/wdecoster/chopper
